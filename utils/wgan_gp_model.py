import os
import numpy as np
import tensorflow as tf
import datetime
from tensorflow.summary import create_file_writer


class WGANGPModel:
    def __init__(self, generator, critic, generator_optimizer, critic_optimizer, checkpoint_dir, logs_dir, intermediary_malware_dir, batch_size=32, embedding_size=768, n_critic=5, lambda_gp=10):
        self.generator = generator
        self.critic = critic
        self.generator_optimizer = generator_optimizer
        self.critic_optimizer = critic_optimizer
        self.checkpoint_dir = checkpoint_dir
        self.logs_dir = logs_dir
        self.intermediary_malware_dir = intermediary_malware_dir
        self.batch_size = batch_size
        self.embedding_size = embedding_size
        self.n_critic = n_critic
        self.lambda_gp = lambda_gp
        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.generator_optimizer,
                                              critic_optimizer=self.critic_optimizer,
                                              generator=self.generator,
                                              critic=self.critic)
        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, checkpoint_dir, max_to_keep=5)

        
    # WGAN-GP loss function
    def critic_loss(self, critic_real, critic_fake):
        return tf.reduce_mean(critic_fake) - tf.reduce_mean(critic_real)

    def generator_loss(self, critic_fake):
        return -tf.reduce_mean(critic_fake)

    # Gradient Penalty
    def gradient_penalty(self, batch_size, real_data, fake_data, critic):
        fake_data = tf.expand_dims(fake_data, axis=-1)
        alpha = tf.random.normal([batch_size, 1, 1], 0.0, 1.0)
        diff = fake_data - real_data
        interpolated = real_data + alpha * diff

        with tf.GradientTape() as gp_tape:
            gp_tape.watch(interpolated)
            pred = critic(interpolated, training=True)

        grads = gp_tape.gradient(pred, [interpolated])[0]
        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))
        gp = tf.reduce_mean((norm - 1.0) ** 2)

        return gp

    # Function to save checkpoints
    def save_checkpoint(self, epoch):
        checkpoint_save_path = self.checkpoint_manager.save(epoch)
        print(f"Checkpoint saved at {checkpoint_save_path} for epoch {epoch}")

    def train_step(self, real_data):
        for _ in range(self.n_critic):
            # Train Critic
            noise = tf.random.normal([self.batch_size, self.embedding_size])
            with tf.GradientTape() as tape:
                fake_data = self.generator(noise, training=True)
                critic_real = self.critic(real_data, training=True)
                critic_fake = self.critic(fake_data, training=True)
                gp = self.gradient_penalty(self.batch_size, real_data, fake_data, self.critic)
                c_loss = self.critic_loss(critic_real, critic_fake) + self.lambda_gp * gp

            critic_gradients = tape.gradient(c_loss, self.critic.trainable_variables)
            self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))

        noise = tf.random.normal([self.batch_size, self.embedding_size])
        # Train Generator
        with tf.GradientTape() as tape:
            fake_data = self.generator(noise, training=True)
            critic_fake = self.critic(fake_data, training=True)
            g_loss = self.generator_loss(critic_fake)

        generator_gradients = tape.gradient(g_loss, self.generator.trainable_variables)
        self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))
        
        return g_loss, c_loss

    def train(self, dataset, epochs):

        curr_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        log_dir = os.path.join(f"{self.logs_dir}/{curr_time}")
        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)


        summary_writer = create_file_writer(log_dir)

        for epoch in range(epochs):

            total_g_loss = 0
            total_c_loss = 0
            num_steps = 0

            for real_data in dataset:
                g_loss, c_loss = self.train_step(real_data)
                total_g_loss += g_loss
                total_c_loss += c_loss
                num_steps += 1

            avg_g_loss = total_g_loss / num_steps
            avg_c_loss = total_c_loss / num_steps

            print(f"Epoch {epoch+1}, Average Generator Loss: {avg_g_loss.numpy()}, Critic Loss: {avg_c_loss.numpy()}")

            with summary_writer.as_default():
                tf.summary.scalar('Average Generator Loss', avg_g_loss, step=epoch)
                tf.summary.scalar('Average Critic Loss', avg_c_loss, step=epoch)
                tf.summary.flush()

            # Save checkpoints every 10 epochs
            if (epoch + 1) % 10 == 0:
                self.checkpoint_manager.save(epoch)
            
            # Save intermediary malware samples every 100 epochs
            if (epoch + 1) % 100 == 0:
                intermediary_malware_samples = self.generate_fake_data(num_samples = 1000)
                np.save(f"{self.intermediary_malware_dir}/intermediary_malware_samples_epoch_{epoch+1}.npy", intermediary_malware_samples)

    
    def restore_checkpoint(self):
        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)
        self.checkpoint.restore(latest_checkpoint)

    def generate_fake_data(self, num_samples=1):
        self.restore_checkpoint()
        noise = tf.random.normal([num_samples, self.embedding_size])
        fake_data = self.generator(noise, training=False)
        return fake_data.numpy()
