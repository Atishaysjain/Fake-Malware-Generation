import os
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
import pandas as pd
import numpy as np


def get_dataframe_from_embeddings(embedding_input_dir):
    dataset = np.load(embedding_input_dir)
    return pd.DataFrame(dataset)


def get_dataset(winwebsec_embeddings, zbot_embeddings, zeroacces_embeddings):
    dataset_winwebsec = get_dataframe_from_embeddings(winwebsec_embeddings)
    dataset_winwebsec['label'] = 0
    dataset_zbot = get_dataframe_from_embeddings(zbot_embeddings)
    dataset_zbot['label'] = 1
    dataset_zeroaccess = get_dataframe_from_embeddings(zeroacces_embeddings)
    dataset_zeroaccess['label'] = 2

    data = pd.concat([dataset_winwebsec, dataset_zbot, dataset_zeroaccess], ignore_index=True)

    return data.drop(columns=['label']), data['label']


def create_model(num_classes=3):
    base_model = tf.keras.applications.ResNet18(
        include_top=False, weights='imagenet', input_shape=(24, 32)
    )
    # Freeze ResNet parameters
    base_model.trainable = False
    
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model


if __name__ == '__main__':
    
    X_original, y_original = get_dataset(
        winwebsec_embeddings = os.path.join(os.getcwd(), "distelBERT_embeddings/Malicia (Big 3 - Opcodes)/winwebsec/tensor_embedding.npy"), 
        zbot_embeddings = os.path.join(os.getcwd(), "distelBERT_embeddings/Malicia (Big 3 - Opcodes)/zbot/tensor_embedding.npy"), 
        zeroacces_embeddings = os.path.join(os.getcwd(), "distelBERT_embeddings/Malicia (Big 3 - Opcodes)/zeroaccess/tensor_embedding.npy")
    )

    X_fake, y_fake = get_dataset(
        winwebsec_embeddings = os.path.join(os.getcwd(), "intermediary_malware_winwebsec/intermediary_malware_samples_epoch_500.npy"),
        zbot_embeddings = os.path.join(os.getcwd(), "intermediary_malware_zbot/intermediary_malware_samples_epoch_500.npy"), 
        zeroacces_embeddings = os.path.join(os.getcwd(), "intermediary_malware_zeroaccess/intermediary_malware_samples_epoch_500.npy")
    )

    print(X_original.shape, y_original.shape, X_fake.shape, y_fake.shape)

    print(X_original.head())
    print(y_original.head())
    print(X_fake.head())
    print(y_fake.head())

    X_original_train, X_original_test, y_original_train, y_original_test = train_test_split(X_original, y_original, test_size=0.2, random_state=42)

    # Classification of fake malware samples into its classes after training classifiers on original malware samples
    # classifiers = {
    #     "Logistic Regression": LogisticRegression(max_iter=1000),
    #     "Random Forest": RandomForestClassifier(),
    #     "Support Vector Machine": SVC(),
    #     "Gradient Boosting": GradientBoostingClassifier()
    # }

    scaler = StandardScaler()
    pca = PCA(n_components=0.95)

    classifiers = {
    "Logistic Regression": Pipeline([
        ('scaler', scaler),
        ('pca', pca),
        ('clf', LogisticRegression(max_iter=1000, solver='saga'))
    ]),
    "Random Forest": RandomForestClassifier(),
    "Support Vector Machine": Pipeline([
        ('scaler', scaler),
        ('pca', pca),
        ('clf', SVC())
    ]),
    "Gradient Boosting": GradientBoostingClassifier()
}

    results = {"Classifier" : [], "Accuracy Original" : [], "Precision Original" : [], "Recall Original" : [], "F1 Score Original" : [], "Accuracy Fake" : [], "Precision Fake" : [], "Recall Fake" : [], "F1 Score Fake" : []}

    for name, clf in classifiers.items():

        clf.fit(X_original_train, y_original_train)
        y_original_pred = clf.predict(X_original_test)
        y_fake_pred = clf.predict(X_fake)

        accuracy_original = accuracy_score(y_original_test, y_original_pred)
        report_original = classification_report(y_original_test, y_original_pred, output_dict=True)
        precision_original = report_original['macro avg']['precision']
        recall_original = report_original['macro avg']['recall']
        f1_score_original = report_original['macro avg']['f1-score']

        accuracy_fake = accuracy_score(y_fake, y_fake_pred)
        report_fake = classification_report(y_fake, y_fake_pred, output_dict=True)
        precision_fake = report_fake['macro avg']['precision']
        recall_fake = report_fake['macro avg']['recall']
        f1_score_fake = report_fake['macro avg']['f1-score']

        results["Classifier"].append(name)
        results["Accuracy Original"].append(accuracy_original)
        results["Precision Original"].append(precision_original)
        results["Recall Original"].append(recall_original)
        results["F1 Score Original"].append(f1_score_original)
        results["Accuracy Fake"].append(accuracy_fake)
        results["Precision Fake"].append(precision_fake)
        results["Recall Fake"].append(recall_fake)
        results["F1 Score Fake"].append(f1_score_fake)

    
    results_df = pd.DataFrame(results)
    print(results_df)
    results_df.to_csv(os.path.join(os.getcwd(), "classfication_results.csv"))



    # X_fake_train, X_fake_test, y_fake_train, y_fake_test = train_test_split(X_fake, y_fake, test_size=0.2, random_state=42)
